# 容器化部署与运维面试题

## 1. 如何设计HMall项目的Docker容器化方案？

### 回答要点：

**容器化架构设计：**

```
前端应用(Nginx) → API网关(Gateway) → 微服务集群 → 数据层(MySQL/Redis)
```

**1. 基础镜像选择与优化：**

**多阶段构建Dockerfile：**
```dockerfile
# 构建阶段
FROM maven:3.8.4-openjdk-11 AS builder

WORKDIR /app

# 复制pom文件，利用Docker缓存
COPY pom.xml .
COPY */pom.xml ./
RUN mvn dependency:go-offline -B

# 复制源码并构建
COPY . .
RUN mvn clean package -DskipTests -B

# 运行阶段
FROM openjdk:11-jre-slim

# 创建非root用户
RUN groupadd -r hmall && useradd -r -g hmall hmall

# 安装必要工具
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# 复制构建产物
COPY --from=builder /app/target/*.jar app.jar

# 设置文件权限
RUN chown -R hmall:hmall /app
USER hmall

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

# JVM参数优化
ENV JAVA_OPTS="-Xms512m -Xmx1g -XX:+UseG1GC -XX:MaxGCPauseMillis=200"

EXPOSE 8080

ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]
```

**2. Docker Compose编排：**
```yaml
# docker-compose.yml
version: '3.8'

services:
  # 数据库
  mysql:
    image: mysql:8.0.23
    container_name: hmall-mysql
    environment:
      MYSQL_ROOT_PASSWORD: 123456
      MYSQL_DATABASE: hmall
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql:/docker-entrypoint-initdb.d
    ports:
      - "3306:3306"
    command: |
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --max-connections=1000
      --innodb-buffer-pool-size=1G
    networks:
      - hmall-network
    restart: unless-stopped

  # Redis缓存
  redis:
    image: redis:6.2-alpine
    container_name: hmall-redis
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - hmall-network
    restart: unless-stopped

  # Nacos注册中心
  nacos:
    image: nacos/nacos-server:v2.1.0
    container_name: hmall-nacos
    environment:
      MODE: standalone
      SPRING_DATASOURCE_PLATFORM: mysql
      MYSQL_SERVICE_HOST: mysql
      MYSQL_SERVICE_DB_NAME: nacos
      MYSQL_SERVICE_USER: root
      MYSQL_SERVICE_PASSWORD: 123456
    volumes:
      - nacos_data:/home/nacos/data
    ports:
      - "8848:8848"
      - "9848:9848"
    depends_on:
      - mysql
    networks:
      - hmall-network
    restart: unless-stopped

  # 用户服务
  user-service:
    build:
      context: ./user-service
      dockerfile: Dockerfile
    container_name: hmall-user-service
    environment:
      SPRING_PROFILES_ACTIVE: docker
      NACOS_SERVER_ADDR: nacos:8848
      MYSQL_HOST: mysql
      REDIS_HOST: redis
    ports:
      - "8081:8080"
    depends_on:
      - mysql
      - redis
      - nacos
    networks:
      - hmall-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # 商品服务
  item-service:
    build:
      context: ./item-service
      dockerfile: Dockerfile
    container_name: hmall-item-service
    environment:
      SPRING_PROFILES_ACTIVE: docker
      NACOS_SERVER_ADDR: nacos:8848
      MYSQL_HOST: mysql
      REDIS_HOST: redis
    ports:
      - "8082:8080"
    depends_on:
      - mysql
      - redis
      - nacos
    networks:
      - hmall-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # API网关
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    container_name: hmall-gateway
    environment:
      SPRING_PROFILES_ACTIVE: docker
      NACOS_SERVER_ADDR: nacos:8848
    ports:
      - "8080:8080"
    depends_on:
      - nacos
      - user-service
      - item-service
    networks:
      - hmall-network
    restart: unless-stopped

  # Nginx反向代理
  nginx:
    image: nginx:1.21-alpine
    container_name: hmall-nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./static:/usr/share/nginx/html
      - nginx_logs:/var/log/nginx
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - gateway
    networks:
      - hmall-network
    restart: unless-stopped

volumes:
  mysql_data:
  redis_data:
  nacos_data:
  nginx_logs:

networks:
  hmall-network:
    driver: bridge
```

**3. 环境配置管理：**
```yaml
# application-docker.yml
spring:
  datasource:
    url: jdbc:mysql://${MYSQL_HOST:mysql}:3306/hmall?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
    username: root
    password: 123456
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
  
  redis:
    host: ${REDIS_HOST:redis}
    port: 6379
    database: 0
    lettuce:
      pool:
        max-active: 20
        max-idle: 8
        min-idle: 2

  cloud:
    nacos:
      server-addr: ${NACOS_SERVER_ADDR:nacos:8848}
      discovery:
        namespace: ${NACOS_NAMESPACE:public}
        group: ${NACOS_GROUP:DEFAULT_GROUP}

logging:
  level:
    com.hmall: debug
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: /app/logs/application.log
    max-size: 100MB
    max-history: 30
```

---

## 2. Nginx在HMall项目中的配置和优化策略？

### 回答要点：

**1. 反向代理配置：**
```nginx
# nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # 日志格式
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    '$request_time $upstream_response_time';
    
    access_log /var/log/nginx/access.log main;
    
    # 性能优化
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    
    # Gzip压缩
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;
    
    # 上游服务器配置
    upstream hmall_gateway {
        least_conn;
        server gateway:8080 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    # 限流配置
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;
    
    include /etc/nginx/conf.d/*.conf;
}
```

**2. 站点配置：**
```nginx
# conf.d/hmall.conf
server {
    listen 80;
    server_name localhost;
    
    # 安全头设置
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    
    # 静态资源
    location /static/ {
        alias /usr/share/nginx/html/static/;
        expires 30d;
        add_header Cache-Control "public, immutable";
        
        # 静态资源压缩
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
    
    # API接口代理
    location /api/ {
        # 限流
        limit_req zone=api burst=20 nodelay;
        
        proxy_pass http://hmall_gateway/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 超时设置
        proxy_connect_timeout 5s;
        proxy_send_timeout 10s;
        proxy_read_timeout 10s;
        
        # 缓冲设置
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        
        # 错误处理
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
    }
    
    # 登录接口特殊限流
    location /api/auth/login {
        limit_req zone=login burst=5 nodelay;
        
        proxy_pass http://hmall_gateway/auth/login;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
    
    # 健康检查
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
    
    # 前端应用
    location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
        try_files $uri $uri/ /index.html;
        
        # 前端资源缓存
        location ~* \.(html)$ {
            expires 1h;
            add_header Cache-Control "public";
        }
    }
    
    # 错误页面
    error_page 404 /404.html;
    error_page 500 502 503 504 /50x.html;
    
    location = /50x.html {
        root /usr/share/nginx/html;
    }
}

# HTTPS配置（生产环境）
server {
    listen 443 ssl http2;
    server_name your-domain.com;
    
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;
    
    # 现代SSL配置
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256;
    ssl_prefer_server_ciphers off;
    
    # HSTS
    add_header Strict-Transport-Security "max-age=63072000" always;
    
    # 其他配置同HTTP
    # ...
}
```

**3. 性能监控配置：**
```nginx
# 状态监控
server {
    listen 8081;
    server_name localhost;
    
    location /nginx_status {
        stub_status on;
        access_log off;
        allow 127.0.0.1;
        allow 172.16.0.0/12;
        deny all;
    }
    
    location /metrics {
        access_log off;
        allow 127.0.0.1;
        allow 172.16.0.0/12;
        deny all;
        
        content_by_lua_block {
            local prometheus = require "resty.prometheus"
            prometheus:collect()
        }
    }
}
```

---

## 3. 如何设计CI/CD流水线实现自动化部署？

### 回答要点：

**CI/CD架构设计：**

```
代码提交 → GitLab CI → 构建镜像 → 推送镜像仓库 → 部署到K8s → 健康检查
```

**1. GitLab CI配置：**
```yaml
# .gitlab-ci.yml
stages:
  - test
  - build
  - deploy-dev
  - deploy-prod

variables:
  DOCKER_REGISTRY: registry.gitlab.com/hmall
  MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"
  MAVEN_CLI_OPTS: "--batch-mode --errors --fail-at-end --show-version"

cache:
  paths:
    - .m2/repository/
    - target/

# 单元测试
test:unit:
  stage: test
  image: maven:3.8.4-openjdk-11
  script:
    - mvn $MAVEN_CLI_OPTS test
  artifacts:
    reports:
      junit:
        - target/surefire-reports/TEST-*.xml
    paths:
      - target/
  coverage: '/Total.*?([0-9]{1,3})%/'
  only:
    - merge_requests
    - develop
    - master

# 代码质量检查
test:sonar:
  stage: test
  image: maven:3.8.4-openjdk-11
  script:
    - mvn $MAVEN_CLI_OPTS sonar:sonar
      -Dsonar.projectKey=hmall
      -Dsonar.host.url=$SONAR_HOST_URL
      -Dsonar.login=$SONAR_TOKEN
  only:
    - merge_requests
    - develop
    - master

# 构建Docker镜像
build:docker:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    # 构建各个服务镜像
    - |
      for service in user-service item-service cart-service trade-service pay-service gateway; do
        echo "Building $service..."
        docker build -t $DOCKER_REGISTRY/$service:$CI_COMMIT_SHA -f $service/Dockerfile $service/
        docker tag $DOCKER_REGISTRY/$service:$CI_COMMIT_SHA $DOCKER_REGISTRY/$service:latest
        docker push $DOCKER_REGISTRY/$service:$CI_COMMIT_SHA
        docker push $DOCKER_REGISTRY/$service:latest
      done
  only:
    - develop
    - master

# 部署到开发环境
deploy:dev:
  stage: deploy-dev
  image: bitnami/kubectl:latest
  environment:
    name: development
    url: https://dev.hmall.com
  before_script:
    - kubectl config use-context dev-cluster
  script:
    - |
      for service in user-service item-service cart-service trade-service pay-service gateway; do
        envsubst < k8s/$service-deployment.yaml | kubectl apply -f -
        kubectl set image deployment/$service $service=$DOCKER_REGISTRY/$service:$CI_COMMIT_SHA -n hmall-dev
        kubectl rollout status deployment/$service -n hmall-dev --timeout=300s
      done
    
    # 健康检查
    - sleep 30
    - kubectl get pods -n hmall-dev
    - |
      for service in user-service item-service cart-service trade-service pay-service gateway; do
        kubectl wait --for=condition=ready pod -l app=$service -n hmall-dev --timeout=300s
      done
  only:
    - develop

# 部署到生产环境
deploy:prod:
  stage: deploy-prod
  image: bitnami/kubectl:latest
  environment:
    name: production
    url: https://hmall.com
  when: manual
  before_script:
    - kubectl config use-context prod-cluster
  script:
    # 蓝绿部署
    - |
      for service in user-service item-service cart-service trade-service pay-service gateway; do
        # 创建新版本部署
        envsubst < k8s/$service-deployment-green.yaml | kubectl apply -f -
        kubectl set image deployment/$service-green $service=$DOCKER_REGISTRY/$service:$CI_COMMIT_SHA -n hmall-prod
        kubectl rollout status deployment/$service-green -n hmall-prod --timeout=600s
        
        # 健康检查
        kubectl wait --for=condition=ready pod -l app=$service-green -n hmall-prod --timeout=300s
        
        # 切换流量
        kubectl patch service $service -p '{"spec":{"selector":{"app":"'$service'-green"}}}' -n hmall-prod
        
        # 删除旧版本
        kubectl delete deployment $service-blue -n hmall-prod --ignore-not-found=true
        
        # 重命名新版本为蓝色
        kubectl patch deployment $service-green -p '{"metadata":{"name":"'$service'-blue"}}' -n hmall-prod
      done
  only:
    - master
```

**2. Kubernetes部署配置：**
```yaml
# k8s/user-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: hmall-prod
  labels:
    app: user-service
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
    spec:
      containers:
      - name: user-service
        image: ${DOCKER_REGISTRY}/user-service:${CI_COMMIT_SHA}
        ports:
        - containerPort: 8080
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "k8s"
        - name: NACOS_SERVER_ADDR
          value: "nacos.hmall-system:8848"
        - name: MYSQL_HOST
          value: "mysql.hmall-system"
        - name: REDIS_HOST
          value: "redis.hmall-system"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: logs
        emptyDir: {}
      imagePullSecrets:
      - name: gitlab-registry
---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: hmall-prod
spec:
  selector:
    app: user-service
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
```

**3. 监控和告警配置：**
```yaml
# monitoring/prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: hmall-alerts
  namespace: hmall-prod
spec:
  groups:
  - name: hmall.rules
    rules:
    # 部署状态监控
    - alert: DeploymentReplicasMismatch
      expr: kube_deployment_spec_replicas != kube_deployment_status_ready_replicas
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "部署副本数不匹配"
        description: "{{ $labels.deployment }} 期望副本数 {{ $value }} 与就绪副本数不匹配"
    
    # Pod重启监控
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod频繁重启"
        description: "{{ $labels.pod }} 在过去15分钟内重启了 {{ $value }} 次"
    
    # 应用健康检查失败
    - alert: ApplicationHealthCheckFailed
      expr: up{job="hmall-services"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "应用健康检查失败"
        description: "{{ $labels.instance }} 健康检查失败超过2分钟"
```

**4. 自动回滚机制：**
```bash
#!/bin/bash
# scripts/auto-rollback.sh

SERVICE_NAME=$1
NAMESPACE=$2
HEALTH_CHECK_URL=$3
MAX_WAIT_TIME=300

echo "开始部署后健康检查: $SERVICE_NAME"

# 等待部署完成
kubectl rollout status deployment/$SERVICE_NAME -n $NAMESPACE --timeout=${MAX_WAIT_TIME}s

if [ $? -ne 0 ]; then
    echo "部署超时，开始回滚"
    kubectl rollout undo deployment/$SERVICE_NAME -n $NAMESPACE
    exit 1
fi

# 健康检查
echo "等待服务启动..."
sleep 30

for i in {1..10}; do
    HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" $HEALTH_CHECK_URL)
    
    if [ "$HTTP_CODE" = "200" ]; then
        echo "健康检查通过"
        exit 0
    fi
    
    echo "健康检查失败，重试 $i/10"
    sleep 10
done

echo "健康检查失败，开始回滚"
kubectl rollout undo deployment/$SERVICE_NAME -n $NAMESPACE
exit 1
```

---

## 4. 如何实现生产环境的日志收集和分析？

### 回答要点：

**日志架构设计：**

```
应用日志 → Filebeat → Logstash → Elasticsearch → Kibana
```

**1. 应用日志配置：**
```yaml
# logback-spring.xml
<configuration>
    <springProfile name="prod">
        <!-- 控制台输出 -->
        <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
            <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
                <providers>
                    <timestamp/>
                    <logLevel/>
                    <loggerName/>
                    <message/>
                    <mdc/>
                    <arguments/>
                    <stackTrace/>
                </providers>
            </encoder>
        </appender>
        
        <!-- 文件输出 -->
        <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <file>/app/logs/application.log</file>
            <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
                <providers>
                    <timestamp/>
                    <logLevel/>
                    <loggerName/>
                    <message/>
                    <mdc/>
                    <arguments/>
                    <stackTrace/>
                </providers>
            </encoder>
            <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                <fileNamePattern>/app/logs/application.%d{yyyy-MM-dd}.%i.log</fileNamePattern>
                <maxFileSize>100MB</maxFileSize>
                <maxHistory>30</maxHistory>
                <totalSizeCap>10GB</totalSizeCap>
            </rollingPolicy>
        </appender>
        
        <root level="INFO">
            <appender-ref ref="CONSOLE"/>
            <appender-ref ref="FILE"/>
        </root>
    </springProfile>
</configuration>
```

**2. 日志收集配置：**
```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /app/logs/*.log
  fields:
    service: hmall
    environment: prod
  fields_under_root: true
  multiline.pattern: '^\d{4}-\d{2}-\d{2}'
  multiline.negate: true
  multiline.match: after
  json.keys_under_root: true
  json.add_error_key: true

processors:
- add_host_metadata:
    when.not.contains.tags: forwarded
- add_docker_metadata: ~
- add_kubernetes_metadata: ~

output.logstash:
  hosts: ["logstash:5044"]

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

**3. Logstash处理配置：**
```ruby
# logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  # 解析JSON日志
  if [message] =~ /^\{/ {
    json {
      source => "message"
    }
  }
  
  # 添加时间戳
  date {
    match => [ "@timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
  }
  
  # 提取请求ID
  if [mdc][traceId] {
    mutate {
      add_field => { "trace_id" => "%{[mdc][traceId]}" }
    }
  }
  
  # 分类日志级别
  if [level] == "ERROR" {
    mutate {
      add_tag => [ "error" ]
    }
  }
  
  # 提取异常信息
  if [stack_trace] {
    mutate {
      add_field => { "has_exception" => "true" }
    }
  }
  
  # 清理不需要的字段
  mutate {
    remove_field => [ "beat", "offset", "prospector" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "hmall-logs-%{+YYYY.MM.dd}"
    template_name => "hmall-logs"
    template => "/usr/share/logstash/templates/hmall-logs.json"
    template_overwrite => true
  }
  
  # 错误日志单独输出
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "hmall-errors-%{+YYYY.MM.dd}"
    }
  }
}
```

**4. Elasticsearch索引模板：**
```json
{
  "index_patterns": ["hmall-logs-*"],
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1,
    "index.refresh_interval": "30s",
    "index.lifecycle.name": "hmall-logs-policy",
    "index.lifecycle.rollover_alias": "hmall-logs"
  },
  "mappings": {
    "properties": {
      "@timestamp": {
        "type": "date"
      },
      "level": {
        "type": "keyword"
      },
      "logger_name": {
        "type": "keyword"
      },
      "message": {
        "type": "text",
        "analyzer": "standard"
      },
      "service": {
        "type": "keyword"
      },
      "environment": {
        "type": "keyword"
      },
      "trace_id": {
        "type": "keyword"
      },
      "stack_trace": {
        "type": "text"
      },
      "has_exception": {
        "type": "boolean"
      }
    }
  }
}
```

**5. Kibana仪表板配置：**
```json
{
  "version": "7.15.0",
  "objects": [
    {
      "id": "hmall-logs-dashboard",
      "type": "dashboard",
      "attributes": {
        "title": "HMall应用日志监控",
        "panelsJSON": "[\n  {\n    \"version\": \"7.15.0\",\n    \"panelIndex\": \"1\",\n    \"gridData\": {\n      \"x\": 0,\n      \"y\": 0,\n      \"w\": 24,\n      \"h\": 15\n    },\n    \"panelRefName\": \"panel_1\"\n  }\n]",
        "optionsJSON": "{\"useMargins\":true,\"syncColors\":false,\"hidePanelTitles\":false}",
        "timeRestore": false,
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}"
        }
      }
    }
  ]
}
```

**6. 日志告警配置：**
```yaml
# elastalert/rules/error-logs.yaml
name: HMall错误日志告警
type: frequency
index: hmall-logs-*
num_events: 10
timeframe:
  minutes: 5

filter:
- term:
    level: "ERROR"

alert:
- "email"
- "dingtalk"

email:
- "ops@hmall.com"

dingtalk:
  dingtalk_webhook: "https://oapi.dingtalk.com/robot/send?access_token=xxx"
  dingtalk_msgtype: "text"

alert_text: |
  HMall应用出现错误日志告警
  
  时间: {0}
  服务: {1}
  错误数量: {2}
  
  最近错误信息:
  {3}

alert_text_args:
  - "@timestamp"
  - "service"
  - "num_matches"
  - "message"

include:
  - "@timestamp"
  - "service"
  - "logger_name"
  - "message"
  - "stack_trace"
```

**日志分析最佳实践：**
1. **结构化日志**：使用JSON格式，便于解析和查询
2. **链路追踪**：通过TraceId关联分布式调用链
3. **日志分级**：合理设置日志级别，避免日志过多
4. **索引管理**：设置生命周期策略，自动删除过期日志
5. **性能监控**：监控日志收集和存储的性能影响

---

## 5. 如何设计灾备和故障恢复方案？

### 回答要点：

**灾备架构设计：**

```
主数据中心 ←→ 同步复制 ←→ 备数据中心
     ↓              ↓
  本地备份      异地备份
```

**1. 数据库备份策略：**
```bash
#!/bin/bash
# scripts/mysql-backup.sh

BACKUP_DIR="/backup/mysql"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# 全量备份（每日）
full_backup() {
    echo "开始全量备份: $DATE"
    
    mysqldump --single-transaction \
              --routines \
              --triggers \
              --all-databases \
              --master-data=2 \
              -h mysql \
              -u backup_user \
              -p$MYSQL_BACKUP_PASSWORD | \
    gzip > $BACKUP_DIR/full_backup_$DATE.sql.gz
    
    if [ $? -eq 0 ]; then
        echo "全量备份完成: full_backup_$DATE.sql.gz"
        
        # 上传到对象存储
        aws s3 cp $BACKUP_DIR/full_backup_$DATE.sql.gz \
                  s3://hmall-backup/mysql/full/
    else
        echo "全量备份失败"
        exit 1
    fi
}

# 增量备份（每小时）
incremental_backup() {
    echo "开始增量备份: $DATE"
    
    # 获取当前binlog位置
    BINLOG_FILE=$(mysql -h mysql -u backup_user -p$MYSQL_BACKUP_PASSWORD \
                  -e "SHOW MASTER STATUS\G" | grep File | awk '{print $2}')
    BINLOG_POS=$(mysql -h mysql -u backup_user -p$MYSQL_BACKUP_PASSWORD \
                 -e "SHOW MASTER STATUS\G" | grep Position | awk '{print $2}')
    
    # 备份binlog
    mysqlbinlog --read-from-remote-server \
                --host=mysql \
                --user=backup_user \
                --password=$MYSQL_BACKUP_PASSWORD \
                --start-position=$LAST_BINLOG_POS \
                $BINLOG_FILE | \
    gzip > $BACKUP_DIR/incremental_backup_$DATE.sql.gz
    
    # 记录位置
    echo "$BINLOG_FILE:$BINLOG_POS" > $BACKUP_DIR/last_backup_position.txt
    
    # 上传到对象存储
    aws s3 cp $BACKUP_DIR/incremental_backup_$DATE.sql.gz \
              s3://hmall-backup/mysql/incremental/
}

# 清理过期备份
cleanup_old_backups() {
    find $BACKUP_DIR -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
    
    # 清理S3过期备份
    aws s3 ls s3://hmall-backup/mysql/full/ | \
    while read -r line; do
        createDate=$(echo $line | awk '{print $1" "$2}')
        createDate=$(date -d"$createDate" +%s)
        olderThan=$(date -d"$RETENTION_DAYS days ago" +%s)
        
        if [[ $createDate -lt $olderThan ]]; then
            fileName=$(echo $line | awk '{print $4}')
            aws s3 rm s3://hmall-backup/mysql/full/$fileName
        fi
    done
}

# 执行备份
case "$1" in
    "full")
        full_backup
        ;;
    "incremental")
        incremental_backup
        ;;
    "cleanup")
        cleanup_old_backups
        ;;
    *)
        echo "Usage: $0 {full|incremental|cleanup}"
        exit 1
        ;;
esac
```

**2. Redis备份策略：**
```bash
#!/bin/bash
# scripts/redis-backup.sh

REDIS_HOST="redis"
REDIS_PORT="6379"
BACKUP_DIR="/backup/redis"
DATE=$(date +%Y%m%d_%H%M%S)

# RDB备份
rdb_backup() {
    echo "开始Redis RDB备份"
    
    # 触发BGSAVE
    redis-cli -h $REDIS_HOST -p $REDIS_PORT BGSAVE
    
    # 等待备份完成
    while [ $(redis-cli -h $REDIS_HOST -p $REDIS_PORT LASTSAVE) -eq $LAST_SAVE ]; do
        sleep 1
    done
    
    # 复制RDB文件
    docker cp redis:/data/dump.rdb $BACKUP_DIR/dump_$DATE.rdb
    
    # 压缩并上传
    gzip $BACKUP_DIR/dump_$DATE.rdb
    aws s3 cp $BACKUP_DIR/dump_$DATE.rdb.gz s3://hmall-backup/redis/
    
    echo "Redis RDB备份完成"
}

# AOF备份
aof_backup() {
    echo "开始Redis AOF备份"
    
    # 重写AOF
    redis-cli -h $REDIS_HOST -p $REDIS_PORT BGREWRITEAOF
    
    # 等待重写完成
    while [ $(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO persistence | grep aof_rewrite_in_progress | cut -d: -f2 | tr -d '\r') -eq 1 ]; do
        sleep 1
    done
    
    # 复制AOF文件
    docker cp redis:/data/appendonly.aof $BACKUP_DIR/appendonly_$DATE.aof
    
    # 压缩并上传
    gzip $BACKUP_DIR/appendonly_$DATE.aof
    aws s3 cp $BACKUP_DIR/appendonly_$DATE.aof.gz s3://hmall-backup/redis/
    
    echo "Redis AOF备份完成"
}

rdb_backup
aof_backup
```

**3. 故障恢复脚本：**
```bash
#!/bin/bash
# scripts/disaster-recovery.sh

RECOVERY_TYPE=$1
BACKUP_DATE=$2

# MySQL恢复
recover_mysql() {
    echo "开始MySQL数据恢复"
    
    # 下载备份文件
    aws s3 cp s3://hmall-backup/mysql/full/full_backup_$BACKUP_DATE.sql.gz /tmp/
    
    # 停止应用服务
    kubectl scale deployment --replicas=0 -n hmall-prod --all
    
    # 恢复数据
    gunzip -c /tmp/full_backup_$BACKUP_DATE.sql.gz | \
    mysql -h mysql -u root -p$MYSQL_ROOT_PASSWORD
    
    if [ $? -eq 0 ]; then
        echo "MySQL数据恢复成功"
        
        # 重启应用服务
        kubectl scale deployment --replicas=3 -n hmall-prod --all
        
        # 等待服务就绪
        kubectl wait --for=condition=ready pod -l app -n hmall-prod --timeout=300s
        
        echo "应用服务恢复完成"
    else
        echo "MySQL数据恢复失败"
        exit 1
    fi
}

# Redis恢复
recover_redis() {
    echo "开始Redis数据恢复"
    
    # 下载备份文件
    aws s3 cp s3://hmall-backup/redis/dump_$BACKUP_DATE.rdb.gz /tmp/
    
    # 停止Redis
    kubectl scale deployment redis --replicas=0 -n hmall-system
    
    # 恢复数据
    gunzip /tmp/dump_$BACKUP_DATE.rdb.gz
    kubectl cp /tmp/dump_$BACKUP_DATE.rdb redis-0:/data/dump.rdb -n hmall-system
    
    # 重启Redis
    kubectl scale deployment redis --replicas=1 -n hmall-system
    kubectl wait --for=condition=ready pod -l app=redis -n hmall-system --timeout=300s
    
    echo "Redis数据恢复完成"
}

# 完整系统恢复
full_recovery() {
    echo "开始完整系统恢复"
    
    # 1. 恢复基础设施
    kubectl apply -f k8s/infrastructure/
    
    # 2. 恢复数据
    recover_mysql
    recover_redis
    
    # 3. 恢复应用服务
    kubectl apply -f k8s/applications/
    
    # 4. 健康检查
    health_check
    
    echo "完整系统恢复完成"
}

# 健康检查
health_check() {
    echo "开始健康检查"
    
    services=("user-service" "item-service" "cart-service" "trade-service" "pay-service" "gateway")
    
    for service in "${services[@]}"; do
        echo "检查服务: $service"
        
        for i in {1..30}; do
            if kubectl get pods -l app=$service -n hmall-prod | grep Running; then
                echo "$service 服务正常"
                break
            fi
            
            if [ $i -eq 30 ]; then
                echo "$service 服务异常"
                exit 1
            fi
            
            sleep 10
        done
    done
    
    # API健康检查
    for i in {1..10}; do
        HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://gateway.hmall-prod:8080/actuator/health)
        
        if [ "$HTTP_CODE" = "200" ]; then
            echo "API网关健康检查通过"
            break
        fi
        
        if [ $i -eq 10 ]; then
            echo "API网关健康检查失败"
            exit 1
        fi
        
        sleep 30
    done
    
    echo "所有健康检查通过"
}

# 执行恢复
case "$RECOVERY_TYPE" in
    "mysql")
        recover_mysql
        ;;
    "redis")
        recover_redis
        ;;
    "full")
        full_recovery
        ;;
    *)
        echo "Usage: $0 {mysql|redis|full} [backup_date]"
        exit 1
        ;;
esac
```

**4. 监控和告警：**
```yaml
# monitoring/disaster-recovery-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: disaster-recovery-alerts
  namespace: hmall-prod
spec:
  groups:
  - name: backup.rules
    rules:
    # 备份失败告警
    - alert: BackupFailed
      expr: increase(backup_job_failures_total[1h]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "备份任务失败"
        description: "{{ $labels.job }} 备份任务在过去1小时内失败了 {{ $value }} 次"
    
    # 数据库主从延迟
    - alert: MySQLReplicationLag
      expr: mysql_slave_lag_seconds > 300
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "MySQL主从复制延迟"
        description: "MySQL从库延迟 {{ $value }} 秒"
    
    # 存储空间不足
    - alert: BackupStorageSpaceLow
      expr: (backup_storage_used_bytes / backup_storage_total_bytes) * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "备份存储空间不足"
        description: "备份存储使用率达到 {{ $value }}%"
```

**RTO/RPO目标：**
- **RTO (恢复时间目标)**：< 30分钟
- **RPO (恢复点目标)**：< 1小时
- **数据备份频率**：全量备份每日，增量备份每小时
- **备份保留策略**：本地30天，异地90天
- **故障切换时间**：< 5分钟（自动切换）

**定期演练计划：**
1. **月度演练**：数据恢复测试
2. **季度演练**：完整灾备切换
3. **年度演练**：跨地域灾备演练
4. **演练文档**：记录演练过程和改进点